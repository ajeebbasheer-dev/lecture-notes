======
Day 01
======

Commands Used Sofar
====================

Docker
--------

To see docker installed or not::

    systemctl status docker

To see the processes::

    docker ps

To inspect::

    docker inspect <container name>

To get inside a container::

    docker exec -it 9f6c9fb4b0de sh

Expose the port so that the application can be accessible from public::

    docker run -itd --name=<container name> -p 4444:80 nginx 


To take down a container::

    docker rm -f <>

Docker Swarm
^^^^^^^^^^^^^

Initialize swarm::

    docker swarm init

To list the nodes in a swarm (works only from master nodes)::

    docker node ls

To list the network drivers::

    docker network ls

To add a worker to the swarm, in a multi-node setup (from manager)::

    docker swarm join --token <TOKEN(generated by swarm init)>

To add a manager to the swarm, in a multi-manager setup (from manager)::

    docker swarm join-token manager

and::

    docker swarm join --token <TOKEN (generated by  join-token manager)>

To promote a worker (from master)::

    docker node promote worker1

To demote a worker (from master)::

    docker node demote worker1

To decommission the cluster, 

For a worker to leave swarm (from worker nodes)::

    docker swarm leave 

For a master to leave swarm (from master node)::

    docker swarm leave --force

To remove a node from swarm (from master)::

    docker node rm worker2

Note that, the `rm` command will not work if it is a sub-ordinate manager. You need to demote this node first.


Docker service
^^^^^^^^^^^^^^^

To create a docker service::

    docker service create --name=web --replicas=2 -p 8000:80 nginx

To see all running docker services::

    docker service ps -f "desired-state=Running" web

To see services running on a node::

    docker service ps -f "node=Master" web

To see services in a formatted state::

    docker service ps --format "{{.Name}}: {{.Image}}" web

To know on which node it is running::

    docker service ps web

To scale::

    docker service scale web=3

To remove a service::

    docker service rm web

Create a service in global mode::

    docker service create --name=web --mode=global -p 8000:80 nginx



Introduction
=============

- https://docs.docker.com/
- https://kubernetes.io/docs/home/
- Certifications:
  - DCA (Docker Certified Associate) - is completely covered in this course.
  - CKA - Certified Kubernetes Administrator

.. image:: images/day01/summary.png
  :width: 400
  :align: center


H/W Requirements
-----------------

.. image:: images/day01/windows_vmare_lifetime_key.png
  :width: 500
  :align: center

- UTM is a very light hypervisor. Use it for Apple M1 chip users.
- http://mirrors.nxtgen.com/centos-mirror/7.9.2009/isos/x86_64/

.. image:: images/day01/utm.png
  :width: 500
  :align: center


Containerization Technology
----------------------------

- Having one server for one application is not cost-effective. We virtualize the hardware to multiple vms to solve this.
- Cloud: Other way of using virtualization. Provide Everything as a Service.
- Instances you are getting from cloud is also not that cost-effective. Imagine you need 5 instances for an application. You need the same for test environment and support environment. You need 15 instances and you need to pay for all these 15 instances. For instance with Public IP, you need to pay more.
- That is where Containerization comes into the picture.
- **Traditional -> Virtualization -> Cloud -> Containerization**
- **Containerization**: A technology with pre build virtualization (*containerization can also do virtualization, i.e. streamline h/w and s/w resources*). Containerization also has the capability of **encapsulating** your application in the form of container it's own operating environment.
- Development and Devops in one layer.
- There are N number of tools to support containerization: **Podman, Crio, Docker** etc.
- Docker is open source.
- `yum install docker`, `systemctl status docker`
- OKE = Uses customized kubernetes by oracle. So, if you use OPC, you can use limited things.
- To make a container work, you first need an images.
- `docker pull nginx` - contains kernel and nginx installed. 

.. image:: images/day01/pull_images.png
  :width: 400
  :align: center

- This is kept in hub.docker.com, a repository we keep docker images.
- images are read-only templates, just like iso.
- to make it read-write, you need to spin a container out of it. `docker run`

.. image:: images/day01/install_docker.png
  :width: 400
  :align: center

- container is a read-write layer. spin_container.png
- `docker ps`
- `docker inspect nginx`

.. image:: images/day01/dockerps.png
  :width: 400
  :align: center

- Inside the container, it is running on PORT 80.
- to get inside the container: `docker exec -it pavan sh`
- To see IP, you need the ip command inside the container. Since it is not there, you can do it from outside. If the IP is public, you can see it in browser. 

.. image:: images/day01/ipaddress.png
  :width: 400
  :align: center

- You can expose the port so that the curl command will work in any IP, 0.0.0.0/localhost/management ip.

.. image:: images/day01/expose.png
  :width: 400
  :align: center

.. image:: images/day01/browser.png
  :width: 400
  :align: center

.. image:: images/day01/curl_command_localhost.png
  :width: 400
  :align: center

.. image:: images/day01/curl_mgmt_ip.png
  :width: 400
  :align: center

- What happens this container comes down (`docker rm -f pavan1`)?? It is gone no high availability. 
- There comes **Orchestration**: to provide **High Availability** (**scalability, reliability, load balancing**). all these called **Clustering**!!
- Any time a container goes down, another container will be created on same host on or another host.
- Orchestration will keep an eye on the containers.
- There are many : Google's **Kubernetes**, Cloudfront, MESOS, Marathon, **Docker Swarm** etc.


Famous Orchestrators
---------------------

1. Docker's own orchestrator tool called Swarm.

  - Very light orchestrator.
  - Management is very easy. Just one big json file.
  - De-merit: All solutions (CRI, CNI and CSI) are predefined. Swarm will only work with docker. Ingress is inbuilt overlay driver. For storage, they have their own local driver.

2. Google's Kubernetes:

  - Open source orchestrator.
  - None of the solutions are predefined. Any CRI any CNI and any CSI.
  - Disadvantage: it is heavy architecture. Made up of lots of component. Can't manage without guidance. 2GB or more RAM required per instance.
  - OKE - Oracle Kubernetes Engine. CRI, CNI and CSI are oracle specific. for example, storage is ZFS. Only underlying platform changes.

3. Cloud front
4. MESOS: Apache

  - not so heavy, not so light.
  - Demerit: All applications seated inside the MESOS should be apache.

Any Orchestration technology there are 3 predefined rules to ensure that the Orchestration is in place.

- Swarm support only docker.
- Kubernetes supports any container

1. **Calculation of the nodes**.
   
   - In a single node itself you can have orchestration (though not effective). 
   - What happens if the node itself goes down. To have an effective orchestration, you need at least 2.
   - Traditional orchestration technology like oracle racks cluster had an upper limit of 16 nodes. This is not the case in the time of virtualization.

2. **Requires 3 layers**

  - Layer #1: **CRI (Container Runtime Interface)** - Underlying container run time software. Swarm support only docker.
  - Layer #2: **CNI (Container Network Interface)** - Denotes the underlying cluster communication. Nodes have to talk to each other. Applications are sitting inside the container. So there should be a **private** communication (node' heartbeat) and **public** communication (for application to access from outside). Both is given by one only network which is called **Overlay Network**. There are 60-70 drivers are available.  Example: **OVS**, **Ingress**, etc. Overlay driver depends on orchestration. **Swarm supports only Ingress**. **Kubernetes Supports any Overlay driver**.
  - Layer #3: **CSI (Container Storage Interface)** - Denotes the underlying application data to make it persistent. By default the data in a container (also in orchestrator) is transient in nature. You need to make it to persistent. You can use any storage drivers. Storage vendors: Amazon S3, Block storages like cinder, etc.

See the data is gone when I delete the container.

.. image:: images/day01/appdatatemporary.png
  :width: 400
  :align: center

.. important:: Swarm support only docker. Swarm supports only one **Overlay** driver called **Ingress**. Kubernetes supports **ANY container and ANY overlay driver**.

Orchestrator #1: Docker Swarm
================================

With Swarm, IT admins can establish and manage a cluster of Docker nodes as a single virtual system.

# Let's take a 4 node cluster

  .. image:: images/day01/swarm01.png
    :width: 500
    :align: center

There are managers and workers. You can decide which node should be the managers and which nodes are workers.

- Every swarm should have at least one manager node. The node which is initialized first is always a manager node by default.
- default port of manager node: **2377**
- All nodes are in same network to share the token. You can add new workers as long as you have a token.
- Worker: To work, to run the application.
- Manager: assign the work to the worker. default load balanced algorithm. When manager gets first project, it will assign to first worker. All workers will be occupied.
- If one worker goes down, manager will assign the work to other workers.
- Here the manager is a technical manager. It itself can run the applications.
- If the manager goes down, Existing workers will continue it's work. But the distribution is impacted. Load balancing is impacted.
- To solve this, today we use **multi-manager** setup. 

.. image:: images/day01/swarm02.png
  :width: 500
  :align: center

- if there are 2 managers, who will assign the work? The solution is, there is a leader manager. 
- Swarm uses Quorum protection technic to elect the leader manager. This is defined by N=N/2+1. N is the number of managers participating in the election. Quorum is similar to the number of votes you need to get the majority.

.. image:: images/day01/quarum.png
  :width: 300
  :align: center

- Each manager can vote by himself.
- RAFT Consensus technic: The guy who initialized first as a manager will be the leader.
- Docker recommends 7 managers.
- No limit on managers.
- **Quorum should be maintained always for cluster to work.**
- To force a new cluster

.. image:: images/day01/force_new_custer.png
  :width: 300
  :align: center


**Case Scenario #1**:

There are 5 managers. Leader and 2 subordinate managers went down.

- *Will there be any impact?* - It is not maintaining the Quorum (5/2+1=3). only 2 are alive. Cluster failed.
- *What should you do to bring back the cluster?*: Need to bring back any of the manger node which went down.
- *Will he be the leader?*: This is based on RAFT algorithm. The one who initialized first. 

**Case Scenario #2**:

I have 6 manger nodes. Out of these 1 leader and 2 went down.

- *Will there be any impact?* - Quorum=6/2+1=4. Cluster will fail. 
- *What should you do to bring back the cluster?*: We need at least one manager to bring up.

**Case Scenario #3**:

I have 5 manager node. 1 leader and 4 subordinates. One leader and 2 subordinates went down abruptly. Service manager told that we can't bring back due to massive hardware failure.

- *Will there be any impact?*: Cluster will fail.
- *What should you do to bring back the cluster?*: Need to force create a new cluster with new Quorum. Need to reelect. 

**Case Scenario #4**:

6 Node manager swarm setup with one leader. Data center guy suggests a Network split. Equal split up of hardware setup. ie. if 4 switch are there, then 2 here and 2 there. Will you encourage to go for this split up as an architect?

- No. There are 6. It will become 3-3. 
- Quorum of 6 is 4. If we split, 3-3, Quorum is not maintained.

if there are 7 nodes, then if there is a split, then one will be up and maintaining the quorum.

.. important:: Always starts with odd number of managers.

- In a multi-node setup, you have one manager and N number of workers. 
  - Planned: Only manager can promote a worker as a manager when going down.
  - Unplanned: Distribution will be impacted. That's why we go for multi-manager setup using the Quorum  
- In a multi-manager setup, you have many mangers with a leader.

Practicals
-----------

- Installed UTM hypervisor on macbook from https://mac.getutm.app/
- Downloaded CentOS-7-x86_64-DVD-2009.iso from http://centos.excellmedia.net/7.9.2009/isos/x86_64/

Setup Swarm environment
^^^^^^^^^^^^^^^^^^^^^^^^

Requires 2 types of communication (In UTM and Workstation this is already taken care. In virtual box, this facility is not there) .

- You need internet connection in the VM level as well as in the Container network.
- Management network.

.. image:: images/day01/network_mgmt_intnt.png
  :width: 300
  :align: center

**Set up an instance on UTM Hypervisor with Centos7 image**

UTM is very light 

- Click on "Create a new virtual machine"

.. image:: images/day01/utm_vm01.png
  :width: 200
  :align: center

- virtualize. emulate is slow as it takes Everything

.. image:: images/day01/utm_vm02.png
  :width: 200
  :align: center

- Select Other. Don't select linux as it will choose suse.

.. image:: images/day01/utm_vm03.png
  :width: 200
  :align: center

- Browse ISO

.. image:: images/day01/utm_vm04.png
  :width: 200
  :align: center

- Choose 2 cpus and 4GB

.. image:: images/day01/utm_vm05.png
  :width: 200
  :align: center

- Choose 20GB storage for each instance.

.. image:: images/day01/utm_vm06.png
  :width: 200
  :align: center

- Do nothing on shared storage.

.. image:: images/day01/utm_vm07.png
  :width: 200
  :align: center

- Change name to Master and tick Open VM Settings

.. image:: images/day01/utm_vm08.png
  :width: 400
  :align: center

.. image:: images/day01/utm_vm09.png
  :width: 400
  :align: center


.. image:: images/day01/utm_vm10.png
  :width: 400
  :align: center
  

.. image:: images/day01/utm_vm11.png
  :width: 400
  :align: center

- Intel will work only on intel driver. Change it to para virtualized as it will work for everyone.
- random will generate random ip addresses (just for info: for next time you clone it.)

.. image:: images/day01/utm_vm12.png
  :width: 200
  :align: center

- Start installation.

.. image:: images/day01/utm_vm13.png
  :width: 200
  :align: center

.. image:: images/day01/utm_vm14.png
  :width: 200
  :align: center

- Network and Hostname, Installation Destination are the places you need to set. You can see the Alert.

.. image:: images/day01/utm_vm15.png
  :width: 500
  :align: center

.. image:: images/day01/utm_vm16.png
  :width: 500
  :align: center

.. image:: images/day01/utm_vm17.png
  :width: 500
  :align: center

- Network and Hostname, Installation Destination are the places you need to set. You can see the Alert.

.. image:: images/day01/utm_vm18.png
  :width: 400
  :align: center

- Reboot it once done. It will ask for install again. Just Quit and clear the DVD and start

.. image:: images/day01/utm_vm19.png
  :width: 400
  :align: center

Username: root, PW: Welcome1

.. image:: images/day01/utm_vm20.png
  :width: 300
  :align: center

**Now We need to install docker on Centos7**

::

    Pre-Requiste:
    ===============

    1.vi /etc/hosts
     ipaddress hostname     # OMIT this, you will do it with swarm

    2.yum update -y
    3.systemctl disable firewalld
    systemctl stop  firewalld
    iptables -L
    iptables -F
    4.getenforce
    vi /etc/selinux/config
    	SELINUX=disabled (do the change on the 7th line)
    5.yum install net-tools bind-utils wget -y


    Docker-CE install:
    ===================
    yum remove docker docker-common docker-selinux docker-engine
    yum install -y yum-utils device-mapper-persistent-data lvm2
    yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
    yum list docker-ce --showduplicates | sort -r
    yum install -y docker-ce (latest version)
    	or
    yum install -y docker-ce-20.10.14-3.el7  (specific version)
    systemctl enable docker
    systemctl start docker
    systemctl status docker
    poweroff

- `ip a`
- You can see the ip address 


.. image:: images/day01/docker_install01.png
  :width: 300
  :align: center

- Login to VM through terminal to get copy paste option

.. image:: images/day01/docker_install02.png
  :width: 300
  :align: center

Yum update::

    [root@Master ~]# yum update -y
    Loaded plugins: fastestmirror
    Determining fastest mirrors
    * base: mirrors.hostever.com
    ...

You need to disable centos firewall then only centos firewall::

    [root@Master ~]# sudo systemctl disable firewalld
    Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
    Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
    [root@Master ~]# sudi systemctl stop  firewalld^C
    [root@Master ~]# sudo systemctl stop  firewalld
    [root@Master ~]# iptables -L
    Chain INPUT (policy ACCEPT)
    target     prot opt source               destination         

    Chain FORWARD (policy ACCEPT)
    target     prot opt source               destination         

    Chain OUTPUT (policy ACCEPT)
    target     prot opt source               destination         
    [root@Master ~]# iptables -F
    [root@Master ~]# 


Stop security linux. That means. In CENTOS and RHEL, there is a selinx security package. disable this then only you can enable docker se linux.

::

    [root@Master ~]# getenforce
    Enforcing
    [root@Master ~]# vi /etc/selinux/config
    [root@Master ~]# cat /etc/selinux/config

    # This file controls the state of SELinux on the system.
    # SELINUX= can take one of these three values:
    #     enforcing - SELinux security policy is enforced.
    #     permissive - SELinux prints warnings instead of enforcing.
    #     disabled - No SELinux policy is loaded.
    SELINUX=disabled
    # SELINUXTYPE= can take one of three values:
    #     targeted - Targeted processes are protected,
    #     minimum - Modification of targeted policy. Only selected processes are protected. 
    #     mls - Multi Level Security protection.
    SELINUXTYPE=targeted 


::

    yum install net-tools bind-utils wget -y

Docker-CE install includes swarm and all.

::

    yum remove docker docker-common docker-selinux docker-engine
    yum install -y yum-utils device-mapper-persistent-data lvm2
    yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
    yum list docker-ce --showduplicates | sort -r
    yum install -y docker-ce (latest version)
    	or
    yum install -y docker-ce-20.10.14-3.el7  (specific version)

    poweroff



- systemctl enable docker
- systemctl start docker
- systemctl status docker

::

    Complete!
    [root@Master ~]# systemctl enable docker
    Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
    [root@Master ~]# systemctl start docker
    [root@Master ~]# systemctl status docker
    ● docker.service - Docker Application Container Engine
       Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
       Active: active (running) since Mon 2022-09-05 16:28:39 IST; 4s ago
         Docs: https://docs.docker.com
     Main PID: 24768 (dockerd)
        Tasks: 8
       Memory: 33.8M
       CGroup: /system.slice/docker.service
               └─24768 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

    Sep 05 16:28:38 Master dockerd[24768]: time="2022-09-05T16:28:38.983173162+05:30" level=info msg="scheme \"unix\" not registered, fallback to defa...dule=grpc
    Sep 05 16:28:38 Master dockerd[24768]: time="2022-09-05T16:28:38.983201070+05:30" level=info msg="ccResolverWrapper: sending update to cc: {[{unix...dule=grpc
    Sep 05 16:28:38 Master dockerd[24768]: time="2022-09-05T16:28:38.983212265+05:30" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
    Sep 05 16:28:39 Master dockerd[24768]: time="2022-09-05T16:28:39.294561498+05:30" level=info msg="Loading containers: start."
    Sep 05 16:28:39 Master dockerd[24768]: time="2022-09-05T16:28:39.482813897+05:30" level=info msg="Default bridge (docker0) is assigned with an IP ... address"
    Sep 05 16:28:39 Master dockerd[24768]: time="2022-09-05T16:28:39.600771492+05:30" level=info msg="Loading containers: done."
    Sep 05 16:28:39 Master dockerd[24768]: time="2022-09-05T16:28:39.656116427+05:30" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=ove...=20.10.17
    Sep 05 16:28:39 Master dockerd[24768]: time="2022-09-05T16:28:39.657241552+05:30" level=info msg="Daemon has completed initialization"
    Sep 05 16:28:39 Master systemd[1]: Started Docker Application Container Engine.
    Sep 05 16:28:39 Master dockerd[24768]: time="2022-09-05T16:28:39.711831974+05:30" level=info msg="API listen on /var/run/docker.sock"
    Hint: Some lines were ellipsized, use -l to show in full.

This has installed all docker things including swarm::

    [root@Master ~]# docker --help

    Usage:  docker [OPTIONS] COMMAND

    . . .

    Management Commands:
      app*        Docker App (Docker Inc., v0.9.1-beta3)
      builder     Manage builds
      buildx*     Docker Buildx (Docker Inc., v0.8.2-docker)
      config      Manage Docker configs
      container   Manage containers
      context     Manage contexts
      image       Manage images
      manifest    Manage Docker image manifests and manifest lists
      network     Manage networks
      node        Manage Swarm nodes
      plugin      Manage plugins
      scan*       Docker Scan (Docker Inc., v0.17.0)
      secret      Manage Docker secrets
      service     Manage services
      stack       Manage Docker stacks
      swarm       Manage Swarm
      system      Manage Docker
      trust       Manage trust on Docker images
      volume      Manage volumes



**Clone 2 VMs as worker1 and worker2 from Master**


.. image:: images/day01/clone00.png
  :width: 400
  :align: center

  
- Click random to get a new MAC. Otherwise the MAC will same.

.. image:: images/day01/clone01.png
  :width: 400
  :align: center

The `ip a` will show docker as we cloned a vm in which docker is installed.

.. image:: images/day01/ip001.png
  :width: 500
  :align: center

  
.. image:: images/day01/ip002.png
  :width: 500
  :align: center



Master ::

    [2022-09-05 16:40:04]: ~ $ ssh root@192.168.64.3
    root@192.168.64.3's password: 
    Last login: Mon Sep  5 16:32:26 2022
    [root@Master ~]# 
    [root@Master ~]# 

Upon cloning, the worker node host names will still be "Master"

Worker 1::

    [2022-09-05 16:56:08]: ~ $ ssh root@192.168.64.5
    The authenticity of host '192.168.64.5 (192.168.64.5)' can't be established.
    ECDSA key fingerprint is SHA256:PQfu2HGrS0yR1OuQNfKhwJJTvQhmj4Bx1IvEWyLmIlg.
    Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
    Warning: Permanently added '192.168.64.5' (ECDSA) to the list of known hosts.
    root@192.168.64.5's password: 
    Last login: Mon Sep  5 22:15:26 2022
    [root@Master ~]# 

Worker 2::

    [2022-09-05 10:11:33]: ~ $ ssh root@192.168.64.4
    The authenticity of host '192.168.64.4 (192.168.64.4)' can't be established.
    ECDSA key fingerprint is SHA256:PQfu2HGrS0yR1OuQNfKhwJJTvQhmj4Bx1IvEWyLmIlg.
    Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
    Warning: Permanently added '192.168.64.4' (ECDSA) to the list of known hosts.
    root@192.168.64.4's password: 
    Last login: Mon Sep  5 16:46:25 2022
    [root@Master ~]# 

Since you have done a clone ,Change the hostname on worker1 and worker2 - `hostnamectl set-hostname worker#`

Change the hostname on worker1 and worker2 ::

    [2022-09-05 17:04:51]: ~ $ ssh root@192.168.64.4
    root@192.168.64.4's password: 
    Last login: Mon Sep  5 16:53:22 2022 from gateway
    [root@Master ~]# hostnamectl set-hostname worker2
    [root@Master ~]# bash
    [root@worker2 ~]# 


    [2022-09-05 17:04:59]: ~ $ ssh root@192.168.64.5
    root@192.168.64.5's password: 
    Last login: Mon Sep  5 17:04:19 2022
    [root@worker1 ~]# 


Now you need to provide hostname resolutions, Add /etc/hosts entry for hostname resolution on all nodes with all entries PARTICIPATING IN SWARM.

::

    [root@Master ~]# cat /etc/hosts
    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
    192.168.64.3 Master
    192.168.64.5 worker1
    192.168.64.4 worker2

See ping working using hostname::

    [root@Master ~]# ping worker1
    PING worker1 (192.168.64.5) 56(84) bytes of data.
    64 bytes from worker1 (192.168.64.5): icmp_seq=1 ttl=64 time=0.970 ms
    64 bytes from worker1 (192.168.64.5): icmp_seq=2 ttl=64 time=2.48 ms

    --- worker1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1003ms
    rtt min/avg/max/mdev = 0.970/1.725/2.480/0.755 ms

    [root@Master ~]# ping worker2
    PING worker2 (192.168.64.4) 56(84) bytes of data.
    64 bytes from worker2 (192.168.64.4): icmp_seq=1 ttl=64 time=1.90 ms
    64 bytes from worker2 (192.168.64.4): icmp_seq=2 ttl=64 time=2.13 ms
    64 bytes from worker2 (192.168.64.4): icmp_seq=3 ttl=64 time=1.87 ms

    --- worker2 ping statistics ---
    3 packets transmitted, 3 received, 0% packet loss, time 2009ms
    rtt min/avg/max/mdev = 1.870/1.970/2.137/0.118 ms
    [root@Master ~]# 


Do the same on workers::

    [root@worker1 ~]# cat /etc/hosts
    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
    192.168.64.3 Master
    192.168.64.5 worker1
    192.168.64.4 worker2

    [root@worker1 ~]# ping Master
    PING Master (192.168.64.3) 56(84) bytes of data.
    64 bytes from Master (192.168.64.3): icmp_seq=1 ttl=64 time=1.35 ms
    64 bytes from Master (192.168.64.3): icmp_seq=2 ttl=64 time=1.61 ms

    --- Master ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1005ms
    rtt min/avg/max/mdev = 1.354/1.482/1.610/0.128 ms
    [root@worker1 ~]# ping worker2
    PING worker2 (192.168.64.4) 56(84) bytes of data.
    64 bytes from worker2 (192.168.64.4): icmp_seq=1 ttl=64 time=2.14 ms
    64 bytes from worker2 (192.168.64.4): icmp_seq=2 ttl=64 time=1.50 ms

    --- worker2 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1003ms
    rtt min/avg/max/mdev = 1.508/1.824/2.140/0.316 ms
    [root@worker1 ~]# 

::

    [root@worker2 ~]# ping Master
    PING Master (192.168.64.3) 56(84) bytes of data.
    64 bytes from Master (192.168.64.3): icmp_seq=1 ttl=64 time=1.35 ms
    64 bytes from Master (192.168.64.3): icmp_seq=2 ttl=64 time=1.58 ms
    ^C
    --- Master ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1004ms
    rtt min/avg/max/mdev = 1.350/1.465/1.580/0.115 ms
    [root@worker2 ~]# ping worker1
    PING worker1 (192.168.64.5) 56(84) bytes of data.
    64 bytes from worker1 (192.168.64.5): icmp_seq=1 ttl=64 time=0.792 ms
    64 bytes from worker1 (192.168.64.5): icmp_seq=2 ttl=64 time=1.77 ms
    ^C
    --- worker1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1002ms
    rtt min/avg/max/mdev = 0.792/1.282/1.773/0.491 ms
    [root@worker2 ~]# 